{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKzbIfdq0CKr"
   },
   "source": [
    "NLP2_2 https://www.hackerrank.com/challenges/detect-the-domain-name/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание на https://www.hackerrank.com/ выполнено."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain names:\n",
      "askoxford.com;bnsf.com;hydrogencarsnow.com;mrvc.indianrail.gov.in;web.archive.org\n",
      "Result: ok\n",
      "\n",
      "Domain names:\n",
      "b.scorecardresearch.com;books.rediff.com;careers.rediff.com;clients.rediff.com;datastore.rediff.com;datastore01.rediff.com;datastore04.rediff.com;datastore05.rediff.com;dealhojaye.rediff.com;hosting.rediff.com;ia.rediff.com;im.rediff.com;imshopping.rediff.com;imworld.rediff.com;investor.rediff.com;ishare.rediff.com;loc.rediff.com;login.rediff.com;mail.rediff.com;metric.ind.rediff.com;money.rediff.com;mypage.rediff.com;n01.rcdn.in;news.rediff.com;pages.rediff.com;realtime.rediff.com;rediff.com;register.rediff.com;search.rediff.com;shopping.rediff.com;simg.rcdn.in;simg03.rcdn.in;track.rediff.com;w3.org;zarabol.rediff.com\n",
      "Result: ok\n",
      "\n",
      "Domain names:\n",
      "ads.indiatimes.com;ads2book.com;adscontent2.indiatimes.com;advertise.indiatimes.com;ahmedabadmirror.com;astrospeak.com;b.scorecardresearch.com;bangaloremirror.com;base.google.com;boxtv.com;brandcapital.co.in;cmstrendslog.indiatimes.com;content.magicbricks.com;download.macromedia.com;economictimes.indiatimes.com;email.indiatimes.com;entertainment.timesofindia.com;epaper.timesofindia.com;facebook.com;files.adspdbl.com;gaana.com;gogreenindia.co.in;graph.facebook.com;gujarati.economictimes.indiatimes.com;guylife.com;healthmeup.com;hindi.economictimes.indiatimes.com;hotklix.com;ibeat.indiatimes.com;idiva.com;indiatimes.com;itn.ph.affinity.com;itsmyascent.com;luxpresso.com;m.timesofindia.com;macromedia.com;magicbricks.com;maharashtratimes.indiatimes.com;mobile.indiatimes.com;mocolife.com;mumbaimirror.com;myeducationtimes.com;myt.indiatimes.com;mytimes.indiatimes.com;navbharattimes.indiatimes.com;netspiderads2.indiatimes.com;netspiderads3.indiatimes.com;netspideradswc.indiatimes.com;ogp.me;photogallery.indiatimes.com;plus.google.com;profile.live.com;punemirror.in;s3.amazonaws.com;salescrm.indiatimes.com;shopping.indiatimes.com;simplymarry.com;ssl.gstatic.com;static.rewards.indiatimes.com;tags.crwdcntrl.net;techgig.com;technoholik.com;tenders.indiatimes.com;timescity.com;timescontent.com;timescrest.com;timesdeal.com;timesinternet.in;timesjobs.com;timeslog.indiatimes.com;timesnow.live.indiatimes.com;timesnow.tv;timesofindia.hotklix.com;timesofindia.indiatimes.com;timesofindia.speakingtree.in;timesofmoney.com;timestrends.indiatimes.com;twitter.com;vijaykarnataka.indiatimes.com;w3.org;webuild.indiatimes.com;widgets.outbrain.com;ww.itimes.com;yolist.com;zigwheels.com;zoomtv.in\n",
      "Result: ok\n",
      "\n",
      "Domain names:\n",
      "coursera-placement-resumes.s3.amazonaws.com;coursera.org;d2wvvaown1ul17.cloudfront.net;eventing.coursera.org;linkedin.com;ogp.me;s3.amazonaws.com;schema.org;ssl.google-analytics.com;thelearningpoint.net\n",
      "Result: ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Локальная копия задания.\n",
    "\n",
    "def get_unique_domain_names(file_path):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as f:\n",
    "        n = int(f.readline().strip())\n",
    "        urls = set()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            pattern = r'http[s]?:\\/\\/(?:(?:www|ww2)\\.)?([a-z0-9\\-]+(?:\\.[a-z0-9\\-]+)+)[\\/\\?]?'\n",
    "            find_urls = re.findall(pattern, line, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "            if find_urls:\n",
    "                for url in find_urls:\n",
    "                    urls.add(url)\n",
    "\n",
    "        return ';'.join(sorted(list(urls)))\n",
    "\n",
    "def test(domain_names, file_path):\n",
    "    domain_names_test = None\n",
    "    with open(file_path, mode='r', encoding='utf-8') as f:\n",
    "        domain_names_test = f.readline().strip()\n",
    "        \n",
    "    print('Domain names:')\n",
    "    print(domain_names)\n",
    "\n",
    "    if domain_names == domain_names_test:\n",
    "        print('Result: ok')\n",
    "    else:\n",
    "        print('Result: error')\n",
    "\n",
    "    print()\n",
    "        \n",
    "test(get_unique_domain_names('./data/tests/test.txt'), './data/tests/test_out.txt')\n",
    "test(get_unique_domain_names('./data/tests/test_00.txt'), './data/tests/test_00_out.txt')\n",
    "test(get_unique_domain_names('./data/tests/test_01.txt'), './data/tests/test_01_out.txt')\n",
    "test(get_unique_domain_names('./data/tests/test_02.txt'), './data/tests/test_02_out.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op8qPHa8J68_"
   },
   "source": [
    "NLP2_3 (дз1): Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade nltk pandas numpy pymorphy2\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных.\n",
    "df = pd.read_csv('./data/labeled.csv', encoding='utf8')\n",
    "\n",
    "# Берем 100 строк.\n",
    "comments = df[0:100]['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачать список стоп-слов для русского языка.\n",
    "stopwords_ru = nltk.corpus.stopwords.words('russian')\n",
    "# Специальные символы.\n",
    "special_char = [',', ':', ' ', ';', '.', '?', '!', '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1893)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "morph = MorphAnalyzer()\n",
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "# Формирование списка токенов.\n",
    "def process_text(text):\n",
    "    # Удалить пробельные символы и перевести строку в нижний регистр.\n",
    "    s = text.strip().lower()\n",
    "    # Разделить строку на токены.\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    # Фильтрация специальных символов.\n",
    "    tokens = [t for t in tokens if t not in special_char]\n",
    "    # Нормализация токенов.\n",
    "    out_tokens = []\n",
    "    for token in tokens:\n",
    "        # Добавление лемматизации и стеминнга.\n",
    "        token = morph.parse(token)[0].normal_form\n",
    "        token = stemmer.stem(token)\n",
    "        # Фильтрация стоп-слов символов.\n",
    "        if token not in stopwords_ru:\n",
    "            out_tokens.append(token)\n",
    "    return out_tokens\n",
    "\n",
    "# Разделение комментариев на токены.\n",
    "comments_tokens = []\n",
    "for comment in comments:\n",
    "    comments_tokens.append(process_text(comment))\n",
    "\n",
    "# Формирование словаря.\n",
    "vocab = set()\n",
    "for comment_tokens in comments_tokens:\n",
    "    vocab.update(comment_tokens)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Формирование вектора из списка токенов.\n",
    "def vectorize(tokens, vocab):\n",
    "    vector=[]\n",
    "    for w in vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "# Формирование BoW\n",
    "bow = []\n",
    "for comment_tokens in comments_tokens:\n",
    "    bow.append(vectorize(comment_tokens, vocab))\n",
    "    \n",
    "print(np.array(bow).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5DQQnoU1bXY"
   },
   "source": [
    "NLP2_4 (дополнительно) Реализовать классификатор токсичных комментариев tfidf на базе датасета (если не успели на классном занятии)\n",
    "https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments  \n",
    "\n",
    "Дубликат файла: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BRC1-k81pIW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
